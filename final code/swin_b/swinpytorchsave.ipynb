{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9960926,"sourceType":"datasetVersion","datasetId":6104137}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\nclass ContinuedEpochMetricsCallback:\n    def __init__(self, working_path):\n        self.working_path = working_path\n        self.epoch_metrics = []\n        self.epoch_metrics_file = os.path.join(working_path, 'epoch_metrics.json')\n        \n    def save_epoch_metrics(self):\n        with open(self.epoch_metrics_file, 'w') as f:\n            json.dump(self.epoch_metrics, f, indent=4)\n    \n    def on_epoch_start(self):\n        self.epoch_start_time = time.time()\n    \n    def on_epoch_end(self, epoch, logs):\n        epoch_duration = time.time() - self.epoch_start_time\n        actual_epoch = epoch + 1\n        \n        epoch_metrics = {\n            'global_epoch': actual_epoch,\n            'local_epoch': epoch + 1,\n            'duration_minutes': epoch_duration / 60,\n            'accuracy': logs['accuracy'],\n            'val_accuracy': logs['val_accuracy'],\n            'loss': logs['loss'],\n            'val_loss': logs['val_loss'],\n            'precision': logs.get('precision', 0),\n            'val_precision': logs.get('val_precision', 0),\n            'recall': logs.get('recall', 0),\n            'val_recall': logs.get('val_recall', 0)\n        }\n        \n        self.epoch_metrics.append(epoch_metrics)\n        self.save_epoch_metrics()\n        \n        print(f\"\\nGlobal Epoch {actual_epoch} Metrics:\")\n        print(f\"Time taken: {epoch_metrics['duration_minutes']:.2f} minutes\")\n        print(f\"Training Accuracy: {epoch_metrics['accuracy']:.4f}\")\n        print(f\"Validation Accuracy: {epoch_metrics['val_accuracy']:.4f}\")\n        print(f\"Training Loss: {epoch_metrics['loss']:.4f}\")\n        print(f\"Validation Loss: {epoch_metrics['val_loss']:.4f}\")\n        \n        self.plot_metrics()\n\n    def plot_metrics(self):\n        if not self.epoch_metrics:\n            return\n            \n        epochs = [m['global_epoch'] for m in self.epoch_metrics]\n        accuracy = [m['accuracy'] for m in self.epoch_metrics]\n        val_accuracy = [m['val_accuracy'] for m in self.epoch_metrics]\n        loss = [m['loss'] for m in self.epoch_metrics]\n        val_loss = [m['val_loss'] for m in self.epoch_metrics]\n        precision = [m['precision'] for m in self.epoch_metrics]\n        val_precision = [m['val_precision'] for m in self.epoch_metrics]\n        recall = [m['recall'] for m in self.epoch_metrics]\n        val_recall = [m['val_recall'] for m in self.epoch_metrics]\n\n        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n\n        axs[0, 0].plot(epochs, accuracy, label='Training Accuracy')\n        axs[0, 0].plot(epochs, val_accuracy, label='Validation Accuracy')\n        axs[0, 0].set_title('Model Accuracy')\n        axs[0, 0].set_xlabel('Epoch')\n        axs[0, 0].set_ylabel('Accuracy')\n        axs[0, 0].legend()\n\n        axs[0, 1].plot(epochs, loss, label='Training Loss')\n        axs[0, 1].plot(epochs, val_loss, label='Validation Loss')\n        axs[0, 1].set_title('Model Loss')\n        axs[0, 1].set_xlabel('Epoch')\n        axs[0, 1].set_ylabel('Loss')\n        axs[0, 1].legend()\n\n        axs[1, 0].plot(epochs, precision, label='Training Precision')\n        axs[1, 0].plot(epochs, val_precision, label='Validation Precision')\n        axs[1, 0].set_title('Model Precision')\n        axs[1, 0].set_xlabel('Epoch')\n        axs[1, 0].set_ylabel('Precision')\n        axs[1, 0].legend()\n\n        axs[1, 1].plot(epochs, recall, label='Training Recall')\n        axs[1, 1].plot(epochs, val_recall, label='Validation Recall')\n        axs[1, 1].set_title('Model Recall')\n        axs[1, 1].set_xlabel('Epoch')\n        axs[1, 1].set_ylabel('Recall')\n        axs[1, 1].legend()\n\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.working_path, f'training_metrics_epoch_{len(epochs)}.png'))\n        plt.close()\n\nclass BirdClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(BirdClassifier, self).__init__()\n        self.swin = models.swin_b(weights='DEFAULT')\n        \n        # Freeze all parameters initially\n        for param in self.swin.parameters():\n            param.requires_grad = False\n            \n        # Unfreeze the last two stages\n        for layer in [self.swin.features[-1], self.swin.features[-2]]:\n            for param in layer.parameters():\n                param.requires_grad = True\n        \n        # Get the number of features from the last layer\n        num_features = self.swin.head.in_features\n        \n        # Replace the classification head with improved architecture\n        self.swin.head = nn.Sequential(\n            nn.Linear(num_features, 1536),\n            nn.BatchNorm1d(1536),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1536, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, num_classes)\n        )\n        \n    def forward(self, x):\n        return self.swin(x)\n\ndef load_training_state_and_metrics(working_path):\n    try:\n        with open(os.path.join(working_path, 'training_state.json'), 'r') as f:\n            state = json.load(f)\n            global_epoch = state.get('global_epoch', 0)\n            best_val_accuracy = state.get('best_val_accuracy', 0.0)\n            previous_model_path = state.get('model_path', None)\n    except FileNotFoundError:\n        global_epoch = 0\n        best_val_accuracy = 0.0\n        previous_model_path = None\n    \n    try:\n        with open(os.path.join(working_path, 'epoch_metrics.json'), 'r') as f:\n            epoch_metrics = json.load(f)\n    except FileNotFoundError:\n        epoch_metrics = []\n    \n    return global_epoch, best_val_accuracy, previous_model_path, epoch_metrics\n\ndef save_training_state(working_path, global_epoch, best_val_accuracy, model_path):\n    state = {\n        'global_epoch': global_epoch,\n        'best_val_accuracy': best_val_accuracy,\n        'model_path': model_path\n    }\n    with open(os.path.join(working_path, 'training_state.json'), 'w') as f:\n        json.dump(state, f, indent=4)\n\ndef validate_model(model, val_loader, criterion, device):\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            val_loss += loss.item()\n    \n    return val_loss / len(val_loader), correct / total\n\ndef evaluate_model(model, test_loader, class_names, device, working_path):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    test_loss = 0\n    correct = 0\n    total = 0\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    accuracy = correct / total\n    test_loss /= len(test_loader)\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plot_confusion_matrix(cm, class_names, working_path)\n    \n    # Generate classification report\n    report = classification_report(all_labels, all_preds, \n                                 target_names=class_names, \n                                 output_dict=True)\n    \n    # Save classification report\n    with open(os.path.join(working_path, 'classification_report.json'), 'w') as f:\n        json.dump(report, f, indent=4)\n    \n    # Calculate precision and recall\n    precision = report['weighted avg']['precision']\n    recall = report['weighted avg']['recall']\n    \n    test_metrics = {\n        'test_loss': test_loss,\n        'test_accuracy': accuracy,\n        'test_precision': precision,\n        'test_recall': recall\n    }\n    \n    # Save test metrics\n    with open(os.path.join(working_path, 'test_metrics.json'), 'w') as f:\n        json.dump(test_metrics, f, indent=4)\n    \n    return test_metrics\n\ndef plot_confusion_matrix(cm, classes, save_path):\n    plt.figure(figsize=(20, 20))\n    sns.heatmap(cm, xticklabels=classes, yticklabels=classes, \n                annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.xticks(rotation=90)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_path, 'confusion_matrix.png'))\n    plt.close()\n\ndef train_model(model, train_loader, val_loader, device, num_epochs, working_path, \n                global_epoch=0, best_val_accuracy=0.0):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam([\n        {'params': model.swin.features[-2].parameters(), 'lr': 2e-4},\n        {'params': model.swin.features[-1].parameters(), 'lr': 2e-4},\n        {'params': model.swin.head.parameters(), 'lr': 4e-4}\n    ])\n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min',\n        patience=5,\n        factor=0.2,\n        min_lr=1e-7\n    )\n    \n    metrics_callback = ContinuedEpochMetricsCallback(working_path)\n    best_model_path = os.path.join(working_path, 'best_model.pth')\n    \n    patience = 10\n    patience_counter = 0\n    min_delta = 0.0005\n    min_lr_reached_counter = 0\n    \n    for epoch in range(global_epoch, num_epochs):\n        metrics_callback.on_epoch_start()\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Training phase\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            \n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = correct / total\n        \n        # Validation phase\n        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n        \n        scheduler.step(val_loss)\n        \n        if val_acc > best_val_accuracy + min_delta:\n            best_val_accuracy = val_acc\n            torch.save(model.state_dict(), best_model_path)\n            patience_counter = 0\n            min_lr_reached_counter = 0\n        else:\n            patience_counter += 1\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        if current_lr <= 1e-6:\n            min_lr_reached_counter += 1\n        else:\n            min_lr_reached_counter = 0\n        \n        should_stop = (\n            (patience_counter >= patience and min_lr_reached_counter >= 3) or\n            (patience_counter >= patience * 2)\n        )\n        \n        if should_stop:\n            print(f\"\\nEarly stopping triggered after epoch {epoch + 1}\")\n            print(f\"Reason: {'Patience exceeded' if patience_counter >= patience else 'Minimum LR reached' if min_lr_reached_counter >= 3 else 'Target performance reached'}\")\n            break\n        \n        # Log metrics\n        logs = {\n            'accuracy': train_acc,\n            'val_accuracy': val_acc,\n            'loss': train_loss,\n            'val_loss': val_loss,\n            'precision': 0,\n            'val_precision': 0,\n            'recall': 0,\n            'val_recall': 0\n        }\n        \n        metrics_callback.on_epoch_end(epoch, logs)\n        save_training_state(working_path, epoch + 1, best_val_accuracy, best_model_path)\n    \n    return best_val_accuracy\n\n\n\n\ndef main():\n    BASE_PATH = '/kaggle/input/400birds/400BirdSpecies'\n    WORKING_PATH = '/kaggle/working/'\n    TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n    VALID_PATH = os.path.join(BASE_PATH, 'valid')\n    TEST_PATH = os.path.join(BASE_PATH, 'test')\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Optimized data augmentation\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomRotation(20),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomAffine(0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Load datasets\n    train_dataset = ImageFolder(TRAIN_PATH, transform=train_transform)\n    val_dataset = ImageFolder(VALID_PATH, transform=val_transform)\n    test_dataset = ImageFolder(TEST_PATH, transform=val_transform)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n    \n    # Load previous state\n    global_epoch, best_val_accuracy, previous_model_path, epoch_metrics = load_training_state_and_metrics(WORKING_PATH)\n    \n    print(f\"Resuming training from epoch {global_epoch + 1}\")\n    print(f\"Previous best validation accuracy: {best_val_accuracy:.4f}\")\n    \n    # Create model\n    num_classes = len(train_dataset.classes)\n    model = BirdClassifier(num_classes)\n    \n    if os.path.exists(os.path.join(WORKING_PATH, 'best_model.pth')):\n        model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n    \n    model = model.to(device)\n    \n    # Store class names\n    class_names = train_dataset.classes\n    class_mapping = {i: class_name for i, class_name in enumerate(class_names)}\n    with open(os.path.join(WORKING_PATH, 'class_mapping.json'), 'w') as f:\n        json.dump(class_mapping, f, indent=4)\n    \n    # Training with 100 epochs max\n    total_epochs = 100\n    best_val_accuracy = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        device=device,\n        num_epochs=total_epochs,\n        working_path=WORKING_PATH,\n        global_epoch=global_epoch,\n        best_val_accuracy=best_val_accuracy\n    )\n    \n    # Evaluate best model\n    model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n    test_metrics = evaluate_model(model, test_loader, class_names, device, WORKING_PATH)\n    \n    print(\"\\nTest Set Metrics:\")\n    print(f\"Test Loss: {test_metrics['test_loss']:.4f}\")\n    print(f\"Test Accuracy: {test_metrics['test_accuracy']:.4f}\")\n    print(f\"Test Precision: {test_metrics['test_precision']:.4f}\")\n    print(f\"Test Recall: {test_metrics['test_recall']:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}