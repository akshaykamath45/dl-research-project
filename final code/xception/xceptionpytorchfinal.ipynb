{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d1d5d9d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-20T10:42:26.214512Z",
     "iopub.status.busy": "2025-01-20T10:42:26.214199Z",
     "iopub.status.idle": "2025-01-20T18:04:44.421404Z",
     "shell.execute_reply": "2025-01-20T18:04:44.420416Z"
    },
    "papermill": {
     "duration": 26538.215584,
     "end_time": "2025-01-20T18:04:44.426461",
     "exception": false,
     "start_time": "2025-01-20T10:42:26.210877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from epoch 1\n",
      "Previous best validation accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
      "  model = create_fn(\n",
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/xception-43020ad28.pth\" to /root/.cache/torch/hub/checkpoints/xception-43020ad28.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global Epoch 1 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.6523\n",
      "Validation Accuracy: 0.9325\n",
      "Training Loss: 1.5668\n",
      "Validation Loss: 0.2719\n",
      "\n",
      "Global Epoch 2 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.8432\n",
      "Validation Accuracy: 0.9550\n",
      "Training Loss: 0.5757\n",
      "Validation Loss: 0.1611\n",
      "\n",
      "Global Epoch 3 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.8781\n",
      "Validation Accuracy: 0.9555\n",
      "Training Loss: 0.4385\n",
      "Validation Loss: 0.1397\n",
      "\n",
      "Global Epoch 4 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.8965\n",
      "Validation Accuracy: 0.9665\n",
      "Training Loss: 0.3678\n",
      "Validation Loss: 0.1122\n",
      "\n",
      "Global Epoch 5 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9081\n",
      "Validation Accuracy: 0.9590\n",
      "Training Loss: 0.3212\n",
      "Validation Loss: 0.1136\n",
      "\n",
      "Global Epoch 6 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9192\n",
      "Validation Accuracy: 0.9750\n",
      "Training Loss: 0.2824\n",
      "Validation Loss: 0.0985\n",
      "\n",
      "Global Epoch 7 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9250\n",
      "Validation Accuracy: 0.9715\n",
      "Training Loss: 0.2556\n",
      "Validation Loss: 0.0969\n",
      "\n",
      "Global Epoch 8 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9328\n",
      "Validation Accuracy: 0.9725\n",
      "Training Loss: 0.2301\n",
      "Validation Loss: 0.0902\n",
      "\n",
      "Global Epoch 9 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9366\n",
      "Validation Accuracy: 0.9745\n",
      "Training Loss: 0.2155\n",
      "Validation Loss: 0.0955\n",
      "\n",
      "Global Epoch 10 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9409\n",
      "Validation Accuracy: 0.9775\n",
      "Training Loss: 0.1969\n",
      "Validation Loss: 0.0887\n",
      "\n",
      "Global Epoch 11 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9462\n",
      "Validation Accuracy: 0.9805\n",
      "Training Loss: 0.1850\n",
      "Validation Loss: 0.0880\n",
      "\n",
      "Global Epoch 12 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9480\n",
      "Validation Accuracy: 0.9770\n",
      "Training Loss: 0.1762\n",
      "Validation Loss: 0.0971\n",
      "\n",
      "Global Epoch 13 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9518\n",
      "Validation Accuracy: 0.9790\n",
      "Training Loss: 0.1619\n",
      "Validation Loss: 0.1042\n",
      "\n",
      "Global Epoch 14 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9544\n",
      "Validation Accuracy: 0.9795\n",
      "Training Loss: 0.1570\n",
      "Validation Loss: 0.0942\n",
      "\n",
      "Global Epoch 15 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9567\n",
      "Validation Accuracy: 0.9755\n",
      "Training Loss: 0.1455\n",
      "Validation Loss: 0.1056\n",
      "\n",
      "Global Epoch 16 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9577\n",
      "Validation Accuracy: 0.9780\n",
      "Training Loss: 0.1420\n",
      "Validation Loss: 0.0927\n",
      "\n",
      "Global Epoch 17 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9612\n",
      "Validation Accuracy: 0.9795\n",
      "Training Loss: 0.1303\n",
      "Validation Loss: 0.0951\n",
      "\n",
      "Global Epoch 18 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9727\n",
      "Validation Accuracy: 0.9830\n",
      "Training Loss: 0.0915\n",
      "Validation Loss: 0.0830\n",
      "\n",
      "Global Epoch 19 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9773\n",
      "Validation Accuracy: 0.9820\n",
      "Training Loss: 0.0734\n",
      "Validation Loss: 0.0831\n",
      "\n",
      "Global Epoch 20 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9788\n",
      "Validation Accuracy: 0.9820\n",
      "Training Loss: 0.0682\n",
      "Validation Loss: 0.0840\n",
      "\n",
      "Global Epoch 21 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9810\n",
      "Validation Accuracy: 0.9825\n",
      "Training Loss: 0.0594\n",
      "Validation Loss: 0.0813\n",
      "\n",
      "Global Epoch 22 Metrics:\n",
      "Time taken: 10.64 minutes\n",
      "Training Accuracy: 0.9825\n",
      "Validation Accuracy: 0.9820\n",
      "Training Loss: 0.0548\n",
      "Validation Loss: 0.0836\n",
      "\n",
      "Global Epoch 23 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9843\n",
      "Validation Accuracy: 0.9825\n",
      "Training Loss: 0.0509\n",
      "Validation Loss: 0.0815\n",
      "\n",
      "Global Epoch 24 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9840\n",
      "Validation Accuracy: 0.9835\n",
      "Training Loss: 0.0503\n",
      "Validation Loss: 0.0810\n",
      "\n",
      "Global Epoch 25 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9852\n",
      "Validation Accuracy: 0.9810\n",
      "Training Loss: 0.0469\n",
      "Validation Loss: 0.0853\n",
      "\n",
      "Global Epoch 26 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9857\n",
      "Validation Accuracy: 0.9830\n",
      "Training Loss: 0.0456\n",
      "Validation Loss: 0.0825\n",
      "\n",
      "Global Epoch 27 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9871\n",
      "Validation Accuracy: 0.9840\n",
      "Training Loss: 0.0414\n",
      "Validation Loss: 0.0787\n",
      "\n",
      "Global Epoch 28 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9874\n",
      "Validation Accuracy: 0.9840\n",
      "Training Loss: 0.0404\n",
      "Validation Loss: 0.0910\n",
      "\n",
      "Global Epoch 29 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9870\n",
      "Validation Accuracy: 0.9850\n",
      "Training Loss: 0.0422\n",
      "Validation Loss: 0.0864\n",
      "\n",
      "Global Epoch 30 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9874\n",
      "Validation Accuracy: 0.9845\n",
      "Training Loss: 0.0407\n",
      "Validation Loss: 0.0888\n",
      "\n",
      "Global Epoch 31 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9878\n",
      "Validation Accuracy: 0.9860\n",
      "Training Loss: 0.0392\n",
      "Validation Loss: 0.0789\n",
      "\n",
      "Global Epoch 32 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9888\n",
      "Validation Accuracy: 0.9820\n",
      "Training Loss: 0.0346\n",
      "Validation Loss: 0.0893\n",
      "\n",
      "Global Epoch 33 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9894\n",
      "Validation Accuracy: 0.9825\n",
      "Training Loss: 0.0327\n",
      "Validation Loss: 0.0813\n",
      "\n",
      "Global Epoch 34 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9898\n",
      "Validation Accuracy: 0.9830\n",
      "Training Loss: 0.0324\n",
      "Validation Loss: 0.0836\n",
      "\n",
      "Global Epoch 35 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9904\n",
      "Validation Accuracy: 0.9830\n",
      "Training Loss: 0.0298\n",
      "Validation Loss: 0.0846\n",
      "\n",
      "Global Epoch 36 Metrics:\n",
      "Time taken: 10.62 minutes\n",
      "Training Accuracy: 0.9911\n",
      "Validation Accuracy: 0.9825\n",
      "Training Loss: 0.0278\n",
      "Validation Loss: 0.0805\n",
      "\n",
      "Global Epoch 37 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9904\n",
      "Validation Accuracy: 0.9840\n",
      "Training Loss: 0.0302\n",
      "Validation Loss: 0.0818\n",
      "\n",
      "Global Epoch 38 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9914\n",
      "Validation Accuracy: 0.9840\n",
      "Training Loss: 0.0275\n",
      "Validation Loss: 0.0848\n",
      "\n",
      "Global Epoch 39 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9914\n",
      "Validation Accuracy: 0.9855\n",
      "Training Loss: 0.0275\n",
      "Validation Loss: 0.0803\n",
      "\n",
      "Global Epoch 40 Metrics:\n",
      "Time taken: 10.63 minutes\n",
      "Training Accuracy: 0.9913\n",
      "Validation Accuracy: 0.9840\n",
      "Training Loss: 0.0260\n",
      "Validation Loss: 0.0813\n",
      "\n",
      "Early stopping triggered after epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a79aa895b9e4>:420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Metrics:\n",
      "Test Loss: 0.0260\n",
      "Test Accuracy: 0.9940\n",
      "Test Precision: 0.9948\n",
      "Test Recall: 0.9940\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class ContinuedEpochMetricsCallback:\n",
    "    def __init__(self, working_path):\n",
    "        self.working_path = working_path\n",
    "        self.epoch_metrics = []\n",
    "        self.epoch_metrics_file = os.path.join(working_path, 'epoch_metrics.json')\n",
    "        \n",
    "    def save_epoch_metrics(self):\n",
    "        with open(self.epoch_metrics_file, 'w') as f:\n",
    "            json.dump(self.epoch_metrics, f, indent=4)\n",
    "    \n",
    "    def on_epoch_start(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        actual_epoch = epoch + 1\n",
    "        \n",
    "        epoch_metrics = {\n",
    "            'global_epoch': actual_epoch,\n",
    "            'local_epoch': epoch + 1,\n",
    "            'duration_minutes': epoch_duration / 60,\n",
    "            'accuracy': logs['accuracy'],\n",
    "            'val_accuracy': logs['val_accuracy'],\n",
    "            'loss': logs['loss'],\n",
    "            'val_loss': logs['val_loss'],\n",
    "            'precision': logs.get('precision', 0),\n",
    "            'val_precision': logs.get('val_precision', 0),\n",
    "            'recall': logs.get('recall', 0),\n",
    "            'val_recall': logs.get('val_recall', 0)\n",
    "        }\n",
    "        \n",
    "        self.epoch_metrics.append(epoch_metrics)\n",
    "        self.save_epoch_metrics()\n",
    "        \n",
    "        print(f\"\\nGlobal Epoch {actual_epoch} Metrics:\")\n",
    "        print(f\"Time taken: {epoch_metrics['duration_minutes']:.2f} minutes\")\n",
    "        print(f\"Training Accuracy: {epoch_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Validation Accuracy: {epoch_metrics['val_accuracy']:.4f}\")\n",
    "        print(f\"Training Loss: {epoch_metrics['loss']:.4f}\")\n",
    "        print(f\"Validation Loss: {epoch_metrics['val_loss']:.4f}\")\n",
    "        \n",
    "        self.plot_metrics()\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        if not self.epoch_metrics:\n",
    "            return\n",
    "            \n",
    "        epochs = [m['global_epoch'] for m in self.epoch_metrics]\n",
    "        accuracy = [m['accuracy'] for m in self.epoch_metrics]\n",
    "        val_accuracy = [m['val_accuracy'] for m in self.epoch_metrics]\n",
    "        loss = [m['loss'] for m in self.epoch_metrics]\n",
    "        val_loss = [m['val_loss'] for m in self.epoch_metrics]\n",
    "        precision = [m['precision'] for m in self.epoch_metrics]\n",
    "        val_precision = [m['val_precision'] for m in self.epoch_metrics]\n",
    "        recall = [m['recall'] for m in self.epoch_metrics]\n",
    "        val_recall = [m['val_recall'] for m in self.epoch_metrics]\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        axs[0, 0].plot(epochs, accuracy, label='Training Accuracy')\n",
    "        axs[0, 0].plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "        axs[0, 0].set_title('Model Accuracy')\n",
    "        axs[0, 0].set_xlabel('Epoch')\n",
    "        axs[0, 0].set_ylabel('Accuracy')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        axs[0, 1].plot(epochs, loss, label='Training Loss')\n",
    "        axs[0, 1].plot(epochs, val_loss, label='Validation Loss')\n",
    "        axs[0, 1].set_title('Model Loss')\n",
    "        axs[0, 1].set_xlabel('Epoch')\n",
    "        axs[0, 1].set_ylabel('Loss')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        axs[1, 0].plot(epochs, precision, label='Training Precision')\n",
    "        axs[1, 0].plot(epochs, val_precision, label='Validation Precision')\n",
    "        axs[1, 0].set_title('Model Precision')\n",
    "        axs[1, 0].set_xlabel('Epoch')\n",
    "        axs[1, 0].set_ylabel('Precision')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        axs[1, 1].plot(epochs, recall, label='Training Recall')\n",
    "        axs[1, 1].plot(epochs, val_recall, label='Validation Recall')\n",
    "        axs[1, 1].set_title('Model Recall')\n",
    "        axs[1, 1].set_xlabel('Epoch')\n",
    "        axs[1, 1].set_ylabel('Recall')\n",
    "        axs[1, 1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.working_path, f'training_metrics_epoch_{len(epochs)}.png'))\n",
    "        plt.close()\n",
    "\n",
    "class XceptionBirdClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(XceptionBirdClassifier, self).__init__()\n",
    "        self.xception = timm.create_model('xception', pretrained=True)\n",
    "        \n",
    "        # Freeze all parameters initially\n",
    "        for param in self.xception.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Unfreeze the last three blocks for more capacity\n",
    "        blocks_to_unfreeze = ['block4', 'block3', 'block2']\n",
    "        for name, param in self.xception.named_parameters():\n",
    "            if any(block in name for block in blocks_to_unfreeze):\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Get the number of features from the last layer\n",
    "        num_features = self.xception.num_features\n",
    "        \n",
    "        # Replace the classification head\n",
    "        self.xception.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 1536),\n",
    "            nn.BatchNorm1d(1536),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1536, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.xception(x)\n",
    "\n",
    "def load_training_state_and_metrics(working_path):\n",
    "    try:\n",
    "        with open(os.path.join(working_path, 'training_state.json'), 'r') as f:\n",
    "            state = json.load(f)\n",
    "            global_epoch = state.get('global_epoch', 0)\n",
    "            best_val_accuracy = state.get('best_val_accuracy', 0.0)\n",
    "            previous_model_path = state.get('model_path', None)\n",
    "    except FileNotFoundError:\n",
    "        global_epoch = 0\n",
    "        best_val_accuracy = 0.0\n",
    "        previous_model_path = None\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(working_path, 'epoch_metrics.json'), 'r') as f:\n",
    "            epoch_metrics = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        epoch_metrics = []\n",
    "    \n",
    "    return global_epoch, best_val_accuracy, previous_model_path, epoch_metrics\n",
    "\n",
    "def save_training_state(working_path, global_epoch, best_val_accuracy, model_path):\n",
    "    state = {\n",
    "        'global_epoch': global_epoch,\n",
    "        'best_val_accuracy': best_val_accuracy,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "    with open(os.path.join(working_path, 'training_state.json'), 'w') as f:\n",
    "        json.dump(state, f, indent=4)\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    return val_loss / len(val_loader), correct / total\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names, device, working_path):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plot_confusion_matrix(cm, class_names, working_path)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                 target_names=class_names, \n",
    "                                 output_dict=True)\n",
    "    \n",
    "    # Save classification report\n",
    "    with open(os.path.join(working_path, 'classification_report.json'), 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    \n",
    "    test_metrics = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': accuracy,\n",
    "        'test_precision': precision,\n",
    "        'test_recall': recall\n",
    "    }\n",
    "    \n",
    "    # Save test metrics\n",
    "    with open(os.path.join(working_path, 'test_metrics.json'), 'w') as f:\n",
    "        json.dump(test_metrics, f, indent=4)\n",
    "    \n",
    "    return test_metrics\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, save_path):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(cm, xticklabels=classes, yticklabels=classes, \n",
    "                annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs, working_path, \n",
    "                global_epoch=0, best_val_accuracy=0.0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Xception-specific optimizer configuration with different learning rates for different blocks\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': (p for n, p in model.named_parameters() if 'block2' in n), 'lr': 5e-5},\n",
    "        {'params': (p for n, p in model.named_parameters() if 'block3' in n), 'lr': 1e-4},\n",
    "        {'params': (p for n, p in model.named_parameters() if 'block4' in n), 'lr': 1e-4},\n",
    "        {'params': model.xception.fc.parameters(), 'lr': 3e-4}\n",
    "    ], weight_decay=0.01)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',\n",
    "        patience=5,\n",
    "        factor=0.2,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    metrics_callback = ContinuedEpochMetricsCallback(working_path)\n",
    "    best_model_path = os.path.join(working_path, 'best_model.pth')\n",
    "    \n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    min_delta = 0.0005\n",
    "    \n",
    "    for epoch in range(global_epoch, num_epochs):\n",
    "        metrics_callback.on_epoch_start()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_val_accuracy + min_delta:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after epoch {epoch + 1}\")\n",
    "            break\n",
    "        \n",
    "        # Log metrics\n",
    "        logs = {\n",
    "            'accuracy': train_acc,\n",
    "            'val_accuracy': val_acc,\n",
    "            'loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'precision': 0,\n",
    "            'val_precision': 0,\n",
    "            'recall': 0,\n",
    "            'val_recall': 0\n",
    "        }\n",
    "        \n",
    "        metrics_callback.on_epoch_end(epoch, logs)\n",
    "        save_training_state(working_path, epoch + 1, best_val_accuracy, best_model_path)\n",
    "    \n",
    "    return best_val_accuracy\n",
    "\n",
    "def main():\n",
    "    BASE_PATH = '/kaggle/input/400birds/400BirdSpecies'\n",
    "    WORKING_PATH = '/kaggle/working/'\n",
    "    TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
    "    VALID_PATH = os.path.join(BASE_PATH, 'valid')\n",
    "    TEST_PATH = os.path.join(BASE_PATH, 'test')\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Xception-specific data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),  # Xception preferred input size\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomAffine(0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),  # Xception preferred input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = ImageFolder(TRAIN_PATH, transform=train_transform)\n",
    "    val_dataset = ImageFolder(VALID_PATH, transform=val_transform)\n",
    "    test_dataset = ImageFolder(TEST_PATH, transform=val_transform)\n",
    "    \n",
    "    # Create data loaders with Xception-optimized batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=24, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Load previous state\n",
    "    global_epoch, best_val_accuracy, previous_model_path, epoch_metrics = load_training_state_and_metrics(WORKING_PATH)\n",
    "    \n",
    "    print(f\"Starting training from epoch {global_epoch + 1}\")\n",
    "    print(f\"Previous best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    # Create model\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    model = XceptionBirdClassifier(num_classes)\n",
    "    \n",
    "    if os.path.exists(os.path.join(WORKING_PATH, 'best_model.pth')):\n",
    "        model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Store class names\n",
    "    class_names = train_dataset.classes\n",
    "    class_mapping = {i: class_name for i, class_name in enumerate(class_names)}\n",
    "    with open(os.path.join(WORKING_PATH, 'class_mapping.json'), 'w') as f:\n",
    "        json.dump(class_mapping, f, indent=4)\n",
    "    \n",
    "    # Training\n",
    "    total_epochs = 100\n",
    "    best_val_accuracy = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        num_epochs=total_epochs,\n",
    "        working_path=WORKING_PATH,\n",
    "        global_epoch=global_epoch,\n",
    "        best_val_accuracy=best_val_accuracy\n",
    "    )\n",
    "    \n",
    "    # Evaluate best model\n",
    "    model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n",
    "    test_metrics = evaluate_model(model, test_loader, class_names, device, WORKING_PATH)\n",
    "    \n",
    "    print(\"\\nTest Set Metrics:\")\n",
    "    print(f\"Test Loss: {test_metrics['test_loss']:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_metrics['test_accuracy']:.4f}\")\n",
    "    print(f\"Test Precision: {test_metrics['test_precision']:.4f}\")\n",
    "    print(f\"Test Recall: {test_metrics['test_recall']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6104137,
     "sourceId": 9960926,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26543.700614,
   "end_time": "2025-01-20T18:04:47.374785",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-20T10:42:23.674171",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
