{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9960926,"sourceType":"datasetVersion","datasetId":6104137}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\nclass ContinuedEpochMetricsCallback:\n    def __init__(self, working_path):\n        self.working_path = working_path\n        self.epoch_metrics = []\n        self.epoch_metrics_file = os.path.join(working_path, 'epoch_metrics.json')\n        \n    def save_epoch_metrics(self):\n        with open(self.epoch_metrics_file, 'w') as f:\n            json.dump(self.epoch_metrics, f, indent=4)\n    \n    def on_epoch_start(self):\n        self.epoch_start_time = time.time()\n    \n    def on_epoch_end(self, epoch, logs):\n        epoch_duration = time.time() - self.epoch_start_time\n        actual_epoch = epoch + 1\n        \n        epoch_metrics = {\n            'global_epoch': actual_epoch,\n            'local_epoch': epoch + 1,\n            'duration_minutes': epoch_duration / 60,\n            'accuracy': logs['accuracy'],\n            'val_accuracy': logs['val_accuracy'],\n            'loss': logs['loss'],  # This is now main loss only\n            'val_loss': logs['val_loss'],\n            'aux_loss': logs.get('aux_loss', 0),  # New auxiliary loss metric\n            'combined_loss': logs.get('combined_loss', 0),  # New combined loss metric\n            'precision': logs.get('precision', 0),\n            'val_precision': logs.get('val_precision', 0),\n            'recall': logs.get('recall', 0),\n            'val_recall': logs.get('val_recall', 0)\n        }\n        \n        self.epoch_metrics.append(epoch_metrics)\n        self.save_epoch_metrics()\n        \n        print(f\"\\nGlobal Epoch {actual_epoch} Metrics:\")\n        print(f\"Time taken: {epoch_metrics['duration_minutes']:.2f} minutes\")\n        print(f\"Training Accuracy: {epoch_metrics['accuracy']:.4f}\")\n        print(f\"Validation Accuracy: {epoch_metrics['val_accuracy']:.4f}\")\n        print(f\"Main Loss: {epoch_metrics['loss']:.4f}\")\n        print(f\"Auxiliary Loss: {epoch_metrics['aux_loss']:.4f}\")\n        print(f\"Combined Loss: {epoch_metrics['combined_loss']:.4f}\")\n        print(f\"Validation Loss: {epoch_metrics['val_loss']:.4f}\")\n        \n        self.plot_metrics()\n\n    def plot_metrics(self):\n        if not self.epoch_metrics:\n            return\n            \n        epochs = [m['global_epoch'] for m in self.epoch_metrics]\n        accuracy = [m['accuracy'] for m in self.epoch_metrics]\n        val_accuracy = [m['val_accuracy'] for m in self.epoch_metrics]\n        loss = [m['loss'] for m in self.epoch_metrics]\n        val_loss = [m['val_loss'] for m in self.epoch_metrics]\n        precision = [m['precision'] for m in self.epoch_metrics]\n        val_precision = [m['val_precision'] for m in self.epoch_metrics]\n        recall = [m['recall'] for m in self.epoch_metrics]\n        val_recall = [m['val_recall'] for m in self.epoch_metrics]\n\n        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n\n        axs[0, 0].plot(epochs, accuracy, label='Training Accuracy')\n        axs[0, 0].plot(epochs, val_accuracy, label='Validation Accuracy')\n        axs[0, 0].set_title('Model Accuracy')\n        axs[0, 0].set_xlabel('Epoch')\n        axs[0, 0].set_ylabel('Accuracy')\n        axs[0, 0].legend()\n\n        axs[0, 1].plot(epochs, loss, label='Training Loss')\n        axs[0, 1].plot(epochs, val_loss, label='Validation Loss')\n        axs[0, 1].set_title('Model Loss')\n        axs[0, 1].set_xlabel('Epoch')\n        axs[0, 1].set_ylabel('Loss')\n        axs[0, 1].legend()\n\n        axs[1, 0].plot(epochs, precision, label='Training Precision')\n        axs[1, 0].plot(epochs, val_precision, label='Validation Precision')\n        axs[1, 0].set_title('Model Precision')\n        axs[1, 0].set_xlabel('Epoch')\n        axs[1, 0].set_ylabel('Precision')\n        axs[1, 0].legend()\n\n        axs[1, 1].plot(epochs, recall, label='Training Recall')\n        axs[1, 1].plot(epochs, val_recall, label='Validation Recall')\n        axs[1, 1].set_title('Model Recall')\n        axs[1, 1].set_xlabel('Epoch')\n        axs[1, 1].set_ylabel('Recall')\n        axs[1, 1].legend()\n\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.working_path, f'training_metrics_epoch_{len(epochs)}.png'))\n        plt.close()\n\nclass BirdClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(BirdClassifier, self).__init__()\n        self.inception = models.inception_v3(weights='DEFAULT')\n        \n        # Selectively unfreeze later layers\n        for param in self.inception.parameters():\n            param.requires_grad = False\n            \n        # Unfreeze the last two Mixed layers\n        for layer in [self.inception.Mixed_7a, self.inception.Mixed_7b, self.inception.Mixed_7c]:\n            for param in layer.parameters():\n                param.requires_grad = True\n            \n        # Replace the final layer with improved architecture\n        num_features = self.inception.fc.in_features\n        self.inception.fc = nn.Sequential(\n            nn.Linear(num_features, 1536),\n            nn.BatchNorm1d(1536),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1536, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, num_classes)\n        )\n        \n    def forward(self, x):\n        if self.training:\n            logits, aux_logits = self.inception(x)\n            return logits, aux_logits\n        return self.inception(x)\n\ndef load_training_state_and_metrics(working_path):\n    try:\n        with open(os.path.join(working_path, 'training_state.json'), 'r') as f:\n            state = json.load(f)\n            global_epoch = state.get('global_epoch', 0)\n            best_val_accuracy = state.get('best_val_accuracy', 0.0)\n            previous_model_path = state.get('model_path', None)\n    except FileNotFoundError:\n        global_epoch = 0\n        best_val_accuracy = 0.0\n        previous_model_path = None\n    \n    try:\n        with open(os.path.join(working_path, 'epoch_metrics.json'), 'r') as f:\n            epoch_metrics = json.load(f)\n    except FileNotFoundError:\n        epoch_metrics = []\n    \n    return global_epoch, best_val_accuracy, previous_model_path, epoch_metrics\n\ndef save_training_state(working_path, global_epoch, best_val_accuracy, model_path):\n    state = {\n        'global_epoch': global_epoch,\n        'best_val_accuracy': best_val_accuracy,\n        'model_path': model_path\n    }\n    with open(os.path.join(working_path, 'training_state.json'), 'w') as f:\n        json.dump(state, f, indent=4)\n\ndef validate_model(model, val_loader, criterion, device):\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            val_loss += loss.item()\n    \n    return val_loss / len(val_loader), correct / total\ndef evaluate_model(model, test_loader, class_names, device, working_path):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    test_loss = 0\n    correct = 0\n    total = 0\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    accuracy = correct / total\n    test_loss /= len(test_loader)\n    \n    # Calculate confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plot_confusion_matrix(cm, class_names, working_path)\n    \n    # Generate classification report\n    report = classification_report(all_labels, all_preds, \n                                 target_names=class_names, \n                                 output_dict=True)\n    \n    # Save classification report\n    with open(os.path.join(working_path, 'classification_report.json'), 'w') as f:\n        json.dump(report, f, indent=4)\n    \n    # Calculate precision and recall\n    precision = report['weighted avg']['precision']\n    recall = report['weighted avg']['recall']\n    \n    test_metrics = {\n        'test_loss': test_loss,\n        'test_accuracy': accuracy,\n        'test_precision': precision,\n        'test_recall': recall\n    }\n    \n    # Save test metrics\n    with open(os.path.join(working_path, 'test_metrics.json'), 'w') as f:\n        json.dump(test_metrics, f, indent=4)\n    \n    return test_metrics\ndef plot_confusion_matrix(cm, classes, save_path):\n    plt.figure(figsize=(20, 20))\n    sns.heatmap(cm, xticklabels=classes, yticklabels=classes, \n                annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.xticks(rotation=90)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_path, 'confusion_matrix.png'))\n    plt.close()\n    \ndef train_model(model, train_loader, val_loader, device, num_epochs, working_path, \n                global_epoch=0, best_val_accuracy=0.0):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam([\n        {'params': model.inception.Mixed_7b.parameters(), 'lr': 2e-4},\n        {'params': model.inception.Mixed_7c.parameters(), 'lr': 2e-4},\n        {'params': model.inception.fc.parameters(), 'lr': 4e-4}\n    ])\n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min',\n        patience=5,\n        factor=0.2,\n        min_lr=1e-7\n    )\n    \n    metrics_callback = ContinuedEpochMetricsCallback(working_path)\n    best_model_path = os.path.join(working_path, 'best_model.pth')\n    \n    patience = 10\n    patience_counter = 0\n    min_delta = 0.0005\n    min_lr_reached_counter = 0\n    \n    for epoch in range(global_epoch, num_epochs):\n        metrics_callback.on_epoch_start()\n        model.train()\n        running_main_loss = 0.0\n        running_aux_loss = 0.0\n        running_combined_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Training phase\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs, aux_outputs = model(images)\n            \n            main_loss = criterion(outputs, labels)\n            aux_loss = criterion(aux_outputs, labels)\n            combined_loss = main_loss + 0.3 * aux_loss\n            \n            combined_loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            running_main_loss += main_loss.item()\n            running_aux_loss += aux_loss.item()\n            running_combined_loss += combined_loss.item()\n            \n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_main_loss = running_main_loss / len(train_loader)\n        train_aux_loss = running_aux_loss / len(train_loader)\n        train_combined_loss = running_combined_loss / len(train_loader)\n        train_acc = correct / total\n        \n        # Validation phase\n        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n        \n        scheduler.step(val_loss)\n        \n        if val_acc > best_val_accuracy + min_delta:\n            best_val_accuracy = val_acc\n            torch.save(model.state_dict(), best_model_path)\n            patience_counter = 0\n            min_lr_reached_counter = 0\n        else:\n            patience_counter += 1\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        if current_lr <= 1e-6:\n            min_lr_reached_counter += 1\n        else:\n            min_lr_reached_counter = 0\n        \n        should_stop = (\n            (patience_counter >= patience and min_lr_reached_counter >= 3) or\n            (patience_counter >= patience * 2)\n        )\n        \n        if should_stop:\n            print(f\"\\nEarly stopping triggered after epoch {epoch + 1}\")\n            print(f\"Reason: {'Patience exceeded' if patience_counter >= patience else 'Minimum LR reached' if min_lr_reached_counter >= 3 else 'Target performance reached'}\")\n            break\n        \n        # Log metrics with separate losses\n        logs = {\n            'accuracy': train_acc,\n            'val_accuracy': val_acc,\n            'loss': train_main_loss,  # Only report main loss in metrics\n            'val_loss': val_loss,\n            'aux_loss': train_aux_loss,  # Additional auxiliary loss tracking\n            'combined_loss': train_combined_loss,  # Track combined loss separately\n            'precision': 0,\n            'val_precision': 0,\n            'recall': 0,\n            'val_recall': 0\n        }\n        \n        metrics_callback.on_epoch_end(epoch, logs)\n        save_training_state(working_path, epoch + 1, best_val_accuracy, best_model_path)\n    \n    return best_val_accuracy\ndef plot_confusion_matrix(cm, classes, save_path):\n    plt.figure(figsize=(20, 20))\n    sns.heatmap(cm, xticklabels=classes, yticklabels=classes, \n                annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.xticks(rotation=90)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_path, 'confusion_matrix.png'))\n    plt.close()\ndef main():\n    BASE_PATH = '/kaggle/input/400birds/400BirdSpecies'\n    WORKING_PATH = '/kaggle/working/'\n    TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n    VALID_PATH = os.path.join(BASE_PATH, 'valid')\n    TEST_PATH = os.path.join(BASE_PATH, 'test')\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Optimized data augmentation\n    train_transform = transforms.Compose([\n        transforms.Resize((299, 299)),\n        transforms.RandomRotation(20),  # Increased from 15\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomAffine(0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Added affine transforms\n        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize((299, 299)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Load datasets\n    train_dataset = ImageFolder(TRAIN_PATH, transform=train_transform)\n    val_dataset = ImageFolder(VALID_PATH, transform=val_transform)\n    test_dataset = ImageFolder(TEST_PATH, transform=val_transform)\n    \n    # Create data loaders with increased batch size\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n    \n    # Load previous state\n    global_epoch, best_val_accuracy, previous_model_path, epoch_metrics = load_training_state_and_metrics(WORKING_PATH)\n    \n    print(f\"Resuming training from epoch {global_epoch + 1}\")\n    print(f\"Previous best validation accuracy: {best_val_accuracy:.4f}\")\n    \n    # Create model\n    num_classes = len(train_dataset.classes)\n    model = BirdClassifier(num_classes)\n    \n    if os.path.exists(os.path.join(WORKING_PATH, 'best_model.pth')):\n        model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n    \n    model = model.to(device)\n    \n    # Store class names\n    class_names = train_dataset.classes\n    class_mapping = {i: class_name for i, class_name in enumerate(class_names)}\n    with open(os.path.join(WORKING_PATH, 'class_mapping.json'), 'w') as f:\n        json.dump(class_mapping, f, indent=4)\n    \n    # Training with 100 epochs max\n    total_epochs = 100\n    best_val_accuracy = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        device=device,\n        num_epochs=total_epochs,\n        working_path=WORKING_PATH,\n        global_epoch=global_epoch,\n        best_val_accuracy=best_val_accuracy\n    )\n    \n    # Evaluate best model\n    model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n    test_metrics = evaluate_model(model, test_loader, class_names, device, WORKING_PATH)\n    \n    print(\"\\nTest Set Metrics:\")\n    print(f\"Test Loss: {test_metrics['test_loss']:.4f}\")\n    print(f\"Test Accuracy: {test_metrics['test_accuracy']:.4f}\")\n    print(f\"Test Precision: {test_metrics['test_precision']:.4f}\")\n    print(f\"Test Recall: {test_metrics['test_recall']:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T04:12:42.633237Z","iopub.execute_input":"2025-01-07T04:12:42.633561Z","iopub.status.idle":"2025-01-07T08:21:12.928339Z","shell.execute_reply.started":"2025-01-07T04:12:42.633535Z","shell.execute_reply":"2025-01-07T08:21:12.927440Z"}},"outputs":[{"name":"stdout","text":"Resuming training from epoch 1\nPrevious best validation accuracy: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n100%|██████████| 104M/104M [00:00<00:00, 197MB/s] \n","output_type":"stream"},{"name":"stdout","text":"\nGlobal Epoch 1 Metrics:\nTime taken: 6.65 minutes\nTraining Accuracy: 0.5629\nValidation Accuracy: 0.8980\nMain Loss: 1.9450\nAuxiliary Loss: 8.3581\nCombined Loss: 4.4525\nValidation Loss: 0.3619\n\nGlobal Epoch 2 Metrics:\nTime taken: 6.00 minutes\nTraining Accuracy: 0.7949\nValidation Accuracy: 0.9320\nMain Loss: 0.7565\nAuxiliary Loss: 8.3543\nCombined Loss: 3.2628\nValidation Loss: 0.2063\n\nGlobal Epoch 3 Metrics:\nTime taken: 6.11 minutes\nTraining Accuracy: 0.8392\nValidation Accuracy: 0.9405\nMain Loss: 0.5780\nAuxiliary Loss: 8.3649\nCombined Loss: 3.0874\nValidation Loss: 0.1904\n\nGlobal Epoch 4 Metrics:\nTime taken: 6.04 minutes\nTraining Accuracy: 0.8651\nValidation Accuracy: 0.9380\nMain Loss: 0.4772\nAuxiliary Loss: 8.3644\nCombined Loss: 2.9865\nValidation Loss: 0.1979\n\nGlobal Epoch 5 Metrics:\nTime taken: 5.90 minutes\nTraining Accuracy: 0.8832\nValidation Accuracy: 0.9615\nMain Loss: 0.4083\nAuxiliary Loss: 8.3594\nCombined Loss: 2.9161\nValidation Loss: 0.1370\n\nGlobal Epoch 6 Metrics:\nTime taken: 5.88 minutes\nTraining Accuracy: 0.8964\nValidation Accuracy: 0.9590\nMain Loss: 0.3615\nAuxiliary Loss: 8.3567\nCombined Loss: 2.8685\nValidation Loss: 0.1392\n\nGlobal Epoch 7 Metrics:\nTime taken: 6.05 minutes\nTraining Accuracy: 0.9047\nValidation Accuracy: 0.9535\nMain Loss: 0.3301\nAuxiliary Loss: 8.3591\nCombined Loss: 2.8378\nValidation Loss: 0.1399\n\nGlobal Epoch 8 Metrics:\nTime taken: 6.03 minutes\nTraining Accuracy: 0.9111\nValidation Accuracy: 0.9690\nMain Loss: 0.3026\nAuxiliary Loss: 8.3607\nCombined Loss: 2.8108\nValidation Loss: 0.1131\n\nGlobal Epoch 9 Metrics:\nTime taken: 6.03 minutes\nTraining Accuracy: 0.9210\nValidation Accuracy: 0.9620\nMain Loss: 0.2722\nAuxiliary Loss: 8.3580\nCombined Loss: 2.7796\nValidation Loss: 0.1326\n\nGlobal Epoch 10 Metrics:\nTime taken: 6.09 minutes\nTraining Accuracy: 0.9275\nValidation Accuracy: 0.9710\nMain Loss: 0.2501\nAuxiliary Loss: 8.3640\nCombined Loss: 2.7593\nValidation Loss: 0.1138\n\nGlobal Epoch 11 Metrics:\nTime taken: 6.18 minutes\nTraining Accuracy: 0.9302\nValidation Accuracy: 0.9675\nMain Loss: 0.2370\nAuxiliary Loss: 8.3580\nCombined Loss: 2.7444\nValidation Loss: 0.1262\n\nGlobal Epoch 12 Metrics:\nTime taken: 6.06 minutes\nTraining Accuracy: 0.9347\nValidation Accuracy: 0.9705\nMain Loss: 0.2221\nAuxiliary Loss: 8.3588\nCombined Loss: 2.7297\nValidation Loss: 0.1152\n\nGlobal Epoch 13 Metrics:\nTime taken: 5.92 minutes\nTraining Accuracy: 0.9412\nValidation Accuracy: 0.9685\nMain Loss: 0.2023\nAuxiliary Loss: 8.3599\nCombined Loss: 2.7103\nValidation Loss: 0.1086\n\nGlobal Epoch 14 Metrics:\nTime taken: 5.97 minutes\nTraining Accuracy: 0.9421\nValidation Accuracy: 0.9680\nMain Loss: 0.1940\nAuxiliary Loss: 8.3602\nCombined Loss: 2.7021\nValidation Loss: 0.1324\n\nGlobal Epoch 15 Metrics:\nTime taken: 6.07 minutes\nTraining Accuracy: 0.9467\nValidation Accuracy: 0.9740\nMain Loss: 0.1832\nAuxiliary Loss: 8.3608\nCombined Loss: 2.6914\nValidation Loss: 0.1188\n\nGlobal Epoch 16 Metrics:\nTime taken: 6.04 minutes\nTraining Accuracy: 0.9476\nValidation Accuracy: 0.9695\nMain Loss: 0.1800\nAuxiliary Loss: 8.3579\nCombined Loss: 2.6874\nValidation Loss: 0.1246\n\nGlobal Epoch 17 Metrics:\nTime taken: 6.01 minutes\nTraining Accuracy: 0.9515\nValidation Accuracy: 0.9750\nMain Loss: 0.1661\nAuxiliary Loss: 8.3520\nCombined Loss: 2.6717\nValidation Loss: 0.1284\n\nGlobal Epoch 18 Metrics:\nTime taken: 6.01 minutes\nTraining Accuracy: 0.9517\nValidation Accuracy: 0.9720\nMain Loss: 0.1622\nAuxiliary Loss: 8.3635\nCombined Loss: 2.6712\nValidation Loss: 0.1193\n\nGlobal Epoch 19 Metrics:\nTime taken: 6.03 minutes\nTraining Accuracy: 0.9539\nValidation Accuracy: 0.9665\nMain Loss: 0.1540\nAuxiliary Loss: 8.3600\nCombined Loss: 2.6620\nValidation Loss: 0.1437\n\nGlobal Epoch 20 Metrics:\nTime taken: 6.04 minutes\nTraining Accuracy: 0.9724\nValidation Accuracy: 0.9830\nMain Loss: 0.0879\nAuxiliary Loss: 8.3631\nCombined Loss: 2.5969\nValidation Loss: 0.0955\n\nGlobal Epoch 21 Metrics:\nTime taken: 6.02 minutes\nTraining Accuracy: 0.9794\nValidation Accuracy: 0.9800\nMain Loss: 0.0666\nAuxiliary Loss: 8.3640\nCombined Loss: 2.5758\nValidation Loss: 0.0981\n\nGlobal Epoch 22 Metrics:\nTime taken: 6.04 minutes\nTraining Accuracy: 0.9817\nValidation Accuracy: 0.9790\nMain Loss: 0.0565\nAuxiliary Loss: 8.3628\nCombined Loss: 2.5654\nValidation Loss: 0.0973\n\nGlobal Epoch 23 Metrics:\nTime taken: 6.00 minutes\nTraining Accuracy: 0.9847\nValidation Accuracy: 0.9790\nMain Loss: 0.0516\nAuxiliary Loss: 8.3610\nCombined Loss: 2.5599\nValidation Loss: 0.0934\n\nGlobal Epoch 24 Metrics:\nTime taken: 5.95 minutes\nTraining Accuracy: 0.9847\nValidation Accuracy: 0.9805\nMain Loss: 0.0492\nAuxiliary Loss: 8.3580\nCombined Loss: 2.5566\nValidation Loss: 0.0978\n\nGlobal Epoch 25 Metrics:\nTime taken: 6.04 minutes\nTraining Accuracy: 0.9869\nValidation Accuracy: 0.9815\nMain Loss: 0.0417\nAuxiliary Loss: 8.3537\nCombined Loss: 2.5478\nValidation Loss: 0.0869\n\nGlobal Epoch 26 Metrics:\nTime taken: 6.08 minutes\nTraining Accuracy: 0.9860\nValidation Accuracy: 0.9830\nMain Loss: 0.0417\nAuxiliary Loss: 8.3584\nCombined Loss: 2.5492\nValidation Loss: 0.0964\n\nGlobal Epoch 27 Metrics:\nTime taken: 6.00 minutes\nTraining Accuracy: 0.9883\nValidation Accuracy: 0.9800\nMain Loss: 0.0381\nAuxiliary Loss: 8.3580\nCombined Loss: 2.5455\nValidation Loss: 0.0998\n\nGlobal Epoch 28 Metrics:\nTime taken: 5.98 minutes\nTraining Accuracy: 0.9882\nValidation Accuracy: 0.9830\nMain Loss: 0.0383\nAuxiliary Loss: 8.3636\nCombined Loss: 2.5474\nValidation Loss: 0.1039\n\nGlobal Epoch 29 Metrics:\nTime taken: 6.01 minutes\nTraining Accuracy: 0.9881\nValidation Accuracy: 0.9820\nMain Loss: 0.0374\nAuxiliary Loss: 8.3566\nCombined Loss: 2.5443\nValidation Loss: 0.1031\n\nGlobal Epoch 30 Metrics:\nTime taken: 6.15 minutes\nTraining Accuracy: 0.9895\nValidation Accuracy: 0.9825\nMain Loss: 0.0340\nAuxiliary Loss: 8.3562\nCombined Loss: 2.5409\nValidation Loss: 0.0890\n\nGlobal Epoch 31 Metrics:\nTime taken: 6.08 minutes\nTraining Accuracy: 0.9887\nValidation Accuracy: 0.9795\nMain Loss: 0.0349\nAuxiliary Loss: 8.3590\nCombined Loss: 2.5426\nValidation Loss: 0.1009\n\nGlobal Epoch 32 Metrics:\nTime taken: 6.01 minutes\nTraining Accuracy: 0.9904\nValidation Accuracy: 0.9810\nMain Loss: 0.0308\nAuxiliary Loss: 8.3675\nCombined Loss: 2.5411\nValidation Loss: 0.0930\n\nGlobal Epoch 33 Metrics:\nTime taken: 5.97 minutes\nTraining Accuracy: 0.9919\nValidation Accuracy: 0.9815\nMain Loss: 0.0265\nAuxiliary Loss: 8.3549\nCombined Loss: 2.5329\nValidation Loss: 0.0929\n\nGlobal Epoch 34 Metrics:\nTime taken: 5.97 minutes\nTraining Accuracy: 0.9919\nValidation Accuracy: 0.9805\nMain Loss: 0.0248\nAuxiliary Loss: 8.3583\nCombined Loss: 2.5323\nValidation Loss: 0.0993\n\nGlobal Epoch 35 Metrics:\nTime taken: 6.01 minutes\nTraining Accuracy: 0.9920\nValidation Accuracy: 0.9815\nMain Loss: 0.0257\nAuxiliary Loss: 8.3608\nCombined Loss: 2.5339\nValidation Loss: 0.1058\n\nGlobal Epoch 36 Metrics:\nTime taken: 6.02 minutes\nTraining Accuracy: 0.9926\nValidation Accuracy: 0.9795\nMain Loss: 0.0236\nAuxiliary Loss: 8.3619\nCombined Loss: 2.5322\nValidation Loss: 0.1047\n\nGlobal Epoch 37 Metrics:\nTime taken: 5.99 minutes\nTraining Accuracy: 0.9922\nValidation Accuracy: 0.9810\nMain Loss: 0.0252\nAuxiliary Loss: 8.3534\nCombined Loss: 2.5312\nValidation Loss: 0.1004\n\nGlobal Epoch 38 Metrics:\nTime taken: 6.03 minutes\nTraining Accuracy: 0.9930\nValidation Accuracy: 0.9800\nMain Loss: 0.0209\nAuxiliary Loss: 8.3607\nCombined Loss: 2.5292\nValidation Loss: 0.1003\n\nGlobal Epoch 39 Metrics:\nTime taken: 6.17 minutes\nTraining Accuracy: 0.9930\nValidation Accuracy: 0.9810\nMain Loss: 0.0216\nAuxiliary Loss: 8.3595\nCombined Loss: 2.5294\nValidation Loss: 0.0980\n\nEarly stopping triggered after epoch 40\nReason: Patience exceeded\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-5416522272f3>:453: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n","output_type":"stream"},{"name":"stdout","text":"\nTest Set Metrics:\nTest Loss: 0.0445\nTest Accuracy: 0.9895\nTest Precision: 0.9910\nTest Recall: 0.9895\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}