{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed71244f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-20T12:10:03.106139Z",
     "iopub.status.busy": "2025-01-20T12:10:03.105834Z",
     "iopub.status.idle": "2025-01-20T18:12:48.450816Z",
     "shell.execute_reply": "2025-01-20T18:12:48.449552Z"
    },
    "papermill": {
     "duration": 21765.353878,
     "end_time": "2025-01-20T18:12:48.456431",
     "exception": false,
     "start_time": "2025-01-20T12:10:03.102553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 1\n",
      "Previous best validation accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 122MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global Epoch 1 Metrics:\n",
      "Time taken: 6.57 minutes\n",
      "Training Accuracy: 0.4235\n",
      "Validation Accuracy: 0.8425\n",
      "Loss: 3.3959\n",
      "Validation Loss: 1.2456\n",
      "\n",
      "Global Epoch 2 Metrics:\n",
      "Time taken: 5.79 minutes\n",
      "Training Accuracy: 0.7386\n",
      "Validation Accuracy: 0.9195\n",
      "Loss: 1.3665\n",
      "Validation Loss: 0.5150\n",
      "\n",
      "Global Epoch 3 Metrics:\n",
      "Time taken: 5.78 minutes\n",
      "Training Accuracy: 0.8076\n",
      "Validation Accuracy: 0.9355\n",
      "Loss: 0.8884\n",
      "Validation Loss: 0.3293\n",
      "\n",
      "Global Epoch 4 Metrics:\n",
      "Time taken: 5.74 minutes\n",
      "Training Accuracy: 0.8377\n",
      "Validation Accuracy: 0.9475\n",
      "Loss: 0.6965\n",
      "Validation Loss: 0.2508\n",
      "\n",
      "Global Epoch 5 Metrics:\n",
      "Time taken: 5.86 minutes\n",
      "Training Accuracy: 0.8553\n",
      "Validation Accuracy: 0.9555\n",
      "Loss: 0.5899\n",
      "Validation Loss: 0.2011\n",
      "\n",
      "Global Epoch 6 Metrics:\n",
      "Time taken: 5.94 minutes\n",
      "Training Accuracy: 0.8733\n",
      "Validation Accuracy: 0.9580\n",
      "Loss: 0.5120\n",
      "Validation Loss: 0.1823\n",
      "\n",
      "Global Epoch 7 Metrics:\n",
      "Time taken: 5.88 minutes\n",
      "Training Accuracy: 0.8812\n",
      "Validation Accuracy: 0.9615\n",
      "Loss: 0.4617\n",
      "Validation Loss: 0.1588\n",
      "\n",
      "Global Epoch 8 Metrics:\n",
      "Time taken: 5.81 minutes\n",
      "Training Accuracy: 0.8893\n",
      "Validation Accuracy: 0.9630\n",
      "Loss: 0.4262\n",
      "Validation Loss: 0.1514\n",
      "\n",
      "Global Epoch 9 Metrics:\n",
      "Time taken: 5.82 minutes\n",
      "Training Accuracy: 0.8958\n",
      "Validation Accuracy: 0.9645\n",
      "Loss: 0.3923\n",
      "Validation Loss: 0.1397\n",
      "\n",
      "Global Epoch 10 Metrics:\n",
      "Time taken: 5.83 minutes\n",
      "Training Accuracy: 0.8997\n",
      "Validation Accuracy: 0.9670\n",
      "Loss: 0.3678\n",
      "Validation Loss: 0.1311\n",
      "\n",
      "Global Epoch 11 Metrics:\n",
      "Time taken: 5.89 minutes\n",
      "Training Accuracy: 0.9050\n",
      "Validation Accuracy: 0.9630\n",
      "Loss: 0.3450\n",
      "Validation Loss: 0.1338\n",
      "\n",
      "Global Epoch 12 Metrics:\n",
      "Time taken: 5.85 minutes\n",
      "Training Accuracy: 0.9114\n",
      "Validation Accuracy: 0.9655\n",
      "Loss: 0.3236\n",
      "Validation Loss: 0.1266\n",
      "\n",
      "Global Epoch 13 Metrics:\n",
      "Time taken: 5.82 minutes\n",
      "Training Accuracy: 0.9145\n",
      "Validation Accuracy: 0.9630\n",
      "Loss: 0.3088\n",
      "Validation Loss: 0.1285\n",
      "\n",
      "Global Epoch 14 Metrics:\n",
      "Time taken: 5.85 minutes\n",
      "Training Accuracy: 0.9172\n",
      "Validation Accuracy: 0.9680\n",
      "Loss: 0.2956\n",
      "Validation Loss: 0.1224\n",
      "\n",
      "Global Epoch 15 Metrics:\n",
      "Time taken: 5.84 minutes\n",
      "Training Accuracy: 0.9202\n",
      "Validation Accuracy: 0.9695\n",
      "Loss: 0.2831\n",
      "Validation Loss: 0.1120\n",
      "\n",
      "Global Epoch 16 Metrics:\n",
      "Time taken: 5.73 minutes\n",
      "Training Accuracy: 0.9223\n",
      "Validation Accuracy: 0.9690\n",
      "Loss: 0.2739\n",
      "Validation Loss: 0.1145\n",
      "\n",
      "Global Epoch 17 Metrics:\n",
      "Time taken: 5.81 minutes\n",
      "Training Accuracy: 0.9248\n",
      "Validation Accuracy: 0.9715\n",
      "Loss: 0.2624\n",
      "Validation Loss: 0.1149\n",
      "\n",
      "Global Epoch 18 Metrics:\n",
      "Time taken: 5.74 minutes\n",
      "Training Accuracy: 0.9280\n",
      "Validation Accuracy: 0.9695\n",
      "Loss: 0.2513\n",
      "Validation Loss: 0.1088\n",
      "\n",
      "Global Epoch 19 Metrics:\n",
      "Time taken: 5.64 minutes\n",
      "Training Accuracy: 0.9301\n",
      "Validation Accuracy: 0.9705\n",
      "Loss: 0.2421\n",
      "Validation Loss: 0.1074\n",
      "\n",
      "Global Epoch 20 Metrics:\n",
      "Time taken: 5.74 minutes\n",
      "Training Accuracy: 0.9323\n",
      "Validation Accuracy: 0.9665\n",
      "Loss: 0.2306\n",
      "Validation Loss: 0.1122\n",
      "\n",
      "Global Epoch 21 Metrics:\n",
      "Time taken: 5.79 minutes\n",
      "Training Accuracy: 0.9335\n",
      "Validation Accuracy: 0.9680\n",
      "Loss: 0.2274\n",
      "Validation Loss: 0.1166\n",
      "\n",
      "Global Epoch 22 Metrics:\n",
      "Time taken: 5.77 minutes\n",
      "Training Accuracy: 0.9358\n",
      "Validation Accuracy: 0.9745\n",
      "Loss: 0.2212\n",
      "Validation Loss: 0.1069\n",
      "\n",
      "Global Epoch 23 Metrics:\n",
      "Time taken: 5.93 minutes\n",
      "Training Accuracy: 0.9364\n",
      "Validation Accuracy: 0.9705\n",
      "Loss: 0.2187\n",
      "Validation Loss: 0.1089\n",
      "\n",
      "Global Epoch 24 Metrics:\n",
      "Time taken: 5.76 minutes\n",
      "Training Accuracy: 0.9381\n",
      "Validation Accuracy: 0.9690\n",
      "Loss: 0.2103\n",
      "Validation Loss: 0.1133\n",
      "\n",
      "Global Epoch 25 Metrics:\n",
      "Time taken: 5.76 minutes\n",
      "Training Accuracy: 0.9386\n",
      "Validation Accuracy: 0.9735\n",
      "Loss: 0.2021\n",
      "Validation Loss: 0.1032\n",
      "\n",
      "Global Epoch 26 Metrics:\n",
      "Time taken: 5.73 minutes\n",
      "Training Accuracy: 0.9410\n",
      "Validation Accuracy: 0.9715\n",
      "Loss: 0.1993\n",
      "Validation Loss: 0.1064\n",
      "\n",
      "Global Epoch 27 Metrics:\n",
      "Time taken: 5.93 minutes\n",
      "Training Accuracy: 0.9428\n",
      "Validation Accuracy: 0.9720\n",
      "Loss: 0.1936\n",
      "Validation Loss: 0.1047\n",
      "\n",
      "Global Epoch 28 Metrics:\n",
      "Time taken: 5.89 minutes\n",
      "Training Accuracy: 0.9436\n",
      "Validation Accuracy: 0.9735\n",
      "Loss: 0.1880\n",
      "Validation Loss: 0.0991\n",
      "\n",
      "Global Epoch 29 Metrics:\n",
      "Time taken: 5.96 minutes\n",
      "Training Accuracy: 0.9440\n",
      "Validation Accuracy: 0.9730\n",
      "Loss: 0.1845\n",
      "Validation Loss: 0.1104\n",
      "\n",
      "Global Epoch 30 Metrics:\n",
      "Time taken: 5.85 minutes\n",
      "Training Accuracy: 0.9450\n",
      "Validation Accuracy: 0.9755\n",
      "Loss: 0.1795\n",
      "Validation Loss: 0.1014\n",
      "\n",
      "Global Epoch 31 Metrics:\n",
      "Time taken: 5.81 minutes\n",
      "Training Accuracy: 0.9455\n",
      "Validation Accuracy: 0.9750\n",
      "Loss: 0.1781\n",
      "Validation Loss: 0.0963\n",
      "\n",
      "Global Epoch 32 Metrics:\n",
      "Time taken: 5.87 minutes\n",
      "Training Accuracy: 0.9475\n",
      "Validation Accuracy: 0.9720\n",
      "Loss: 0.1725\n",
      "Validation Loss: 0.1019\n",
      "\n",
      "Global Epoch 33 Metrics:\n",
      "Time taken: 5.80 minutes\n",
      "Training Accuracy: 0.9468\n",
      "Validation Accuracy: 0.9710\n",
      "Loss: 0.1738\n",
      "Validation Loss: 0.1030\n",
      "\n",
      "Global Epoch 34 Metrics:\n",
      "Time taken: 5.92 minutes\n",
      "Training Accuracy: 0.9511\n",
      "Validation Accuracy: 0.9730\n",
      "Loss: 0.1638\n",
      "Validation Loss: 0.0972\n",
      "\n",
      "Global Epoch 35 Metrics:\n",
      "Time taken: 5.71 minutes\n",
      "Training Accuracy: 0.9505\n",
      "Validation Accuracy: 0.9705\n",
      "Loss: 0.1649\n",
      "Validation Loss: 0.0977\n",
      "\n",
      "Global Epoch 36 Metrics:\n",
      "Time taken: 5.72 minutes\n",
      "Training Accuracy: 0.9493\n",
      "Validation Accuracy: 0.9760\n",
      "Loss: 0.1662\n",
      "Validation Loss: 0.0926\n",
      "\n",
      "Global Epoch 37 Metrics:\n",
      "Time taken: 5.70 minutes\n",
      "Training Accuracy: 0.9511\n",
      "Validation Accuracy: 0.9755\n",
      "Loss: 0.1583\n",
      "Validation Loss: 0.0971\n",
      "\n",
      "Global Epoch 38 Metrics:\n",
      "Time taken: 5.86 minutes\n",
      "Training Accuracy: 0.9511\n",
      "Validation Accuracy: 0.9720\n",
      "Loss: 0.1590\n",
      "Validation Loss: 0.0985\n",
      "\n",
      "Global Epoch 39 Metrics:\n",
      "Time taken: 5.90 minutes\n",
      "Training Accuracy: 0.9534\n",
      "Validation Accuracy: 0.9765\n",
      "Loss: 0.1536\n",
      "Validation Loss: 0.0941\n",
      "\n",
      "Global Epoch 40 Metrics:\n",
      "Time taken: 5.85 minutes\n",
      "Training Accuracy: 0.9531\n",
      "Validation Accuracy: 0.9720\n",
      "Loss: 0.1524\n",
      "Validation Loss: 0.1054\n",
      "\n",
      "Global Epoch 41 Metrics:\n",
      "Time taken: 5.78 minutes\n",
      "Training Accuracy: 0.9546\n",
      "Validation Accuracy: 0.9695\n",
      "Loss: 0.1486\n",
      "Validation Loss: 0.1079\n",
      "\n",
      "Global Epoch 42 Metrics:\n",
      "Time taken: 5.90 minutes\n",
      "Training Accuracy: 0.9550\n",
      "Validation Accuracy: 0.9735\n",
      "Loss: 0.1468\n",
      "Validation Loss: 0.0999\n",
      "\n",
      "Global Epoch 43 Metrics:\n",
      "Time taken: 5.86 minutes\n",
      "Training Accuracy: 0.9593\n",
      "Validation Accuracy: 0.9750\n",
      "Loss: 0.1326\n",
      "Validation Loss: 0.0966\n",
      "\n",
      "Global Epoch 44 Metrics:\n",
      "Time taken: 5.88 minutes\n",
      "Training Accuracy: 0.9614\n",
      "Validation Accuracy: 0.9765\n",
      "Loss: 0.1225\n",
      "Validation Loss: 0.0946\n",
      "\n",
      "Global Epoch 45 Metrics:\n",
      "Time taken: 5.88 minutes\n",
      "Training Accuracy: 0.9641\n",
      "Validation Accuracy: 0.9770\n",
      "Loss: 0.1169\n",
      "Validation Loss: 0.0927\n",
      "\n",
      "Global Epoch 46 Metrics:\n",
      "Time taken: 5.65 minutes\n",
      "Training Accuracy: 0.9644\n",
      "Validation Accuracy: 0.9760\n",
      "Loss: 0.1179\n",
      "Validation Loss: 0.0951\n",
      "\n",
      "Global Epoch 47 Metrics:\n",
      "Time taken: 5.91 minutes\n",
      "Training Accuracy: 0.9646\n",
      "Validation Accuracy: 0.9740\n",
      "Loss: 0.1148\n",
      "Validation Loss: 0.0943\n",
      "\n",
      "Global Epoch 48 Metrics:\n",
      "Time taken: 5.73 minutes\n",
      "Training Accuracy: 0.9667\n",
      "Validation Accuracy: 0.9775\n",
      "Loss: 0.1071\n",
      "Validation Loss: 0.0928\n",
      "\n",
      "Global Epoch 49 Metrics:\n",
      "Time taken: 6.01 minutes\n",
      "Training Accuracy: 0.9668\n",
      "Validation Accuracy: 0.9780\n",
      "Loss: 0.1067\n",
      "Validation Loss: 0.0940\n",
      "\n",
      "Global Epoch 50 Metrics:\n",
      "Time taken: 5.73 minutes\n",
      "Training Accuracy: 0.9671\n",
      "Validation Accuracy: 0.9745\n",
      "Loss: 0.1046\n",
      "Validation Loss: 0.0923\n",
      "\n",
      "Global Epoch 51 Metrics:\n",
      "Time taken: 5.90 minutes\n",
      "Training Accuracy: 0.9689\n",
      "Validation Accuracy: 0.9785\n",
      "Loss: 0.1014\n",
      "Validation Loss: 0.0924\n",
      "\n",
      "Global Epoch 52 Metrics:\n",
      "Time taken: 5.77 minutes\n",
      "Training Accuracy: 0.9678\n",
      "Validation Accuracy: 0.9765\n",
      "Loss: 0.1025\n",
      "Validation Loss: 0.0925\n",
      "\n",
      "Global Epoch 53 Metrics:\n",
      "Time taken: 5.83 minutes\n",
      "Training Accuracy: 0.9684\n",
      "Validation Accuracy: 0.9780\n",
      "Loss: 0.1018\n",
      "Validation Loss: 0.0897\n",
      "\n",
      "Global Epoch 54 Metrics:\n",
      "Time taken: 5.83 minutes\n",
      "Training Accuracy: 0.9701\n",
      "Validation Accuracy: 0.9740\n",
      "Loss: 0.1007\n",
      "Validation Loss: 0.0960\n",
      "\n",
      "Global Epoch 55 Metrics:\n",
      "Time taken: 5.84 minutes\n",
      "Training Accuracy: 0.9692\n",
      "Validation Accuracy: 0.9770\n",
      "Loss: 0.0992\n",
      "Validation Loss: 0.0926\n",
      "\n",
      "Global Epoch 56 Metrics:\n",
      "Time taken: 5.82 minutes\n",
      "Training Accuracy: 0.9685\n",
      "Validation Accuracy: 0.9765\n",
      "Loss: 0.1011\n",
      "Validation Loss: 0.0929\n",
      "\n",
      "Global Epoch 57 Metrics:\n",
      "Time taken: 5.94 minutes\n",
      "Training Accuracy: 0.9691\n",
      "Validation Accuracy: 0.9760\n",
      "Loss: 0.1002\n",
      "Validation Loss: 0.0936\n",
      "\n",
      "Global Epoch 58 Metrics:\n",
      "Time taken: 5.82 minutes\n",
      "Training Accuracy: 0.9706\n",
      "Validation Accuracy: 0.9785\n",
      "Loss: 0.0949\n",
      "Validation Loss: 0.0914\n",
      "\n",
      "Global Epoch 59 Metrics:\n",
      "Time taken: 5.73 minutes\n",
      "Training Accuracy: 0.9698\n",
      "Validation Accuracy: 0.9785\n",
      "Loss: 0.0978\n",
      "Validation Loss: 0.0919\n",
      "\n",
      "Global Epoch 60 Metrics:\n",
      "Time taken: 5.88 minutes\n",
      "Training Accuracy: 0.9706\n",
      "Validation Accuracy: 0.9785\n",
      "Loss: 0.0951\n",
      "Validation Loss: 0.0882\n",
      "\n",
      "Early stopping triggered after epoch 61\n",
      "Reason: Patience exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-66856832bbf5>:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Metrics:\n",
      "Test Loss: 0.0449\n",
      "Test Accuracy: 0.9865\n",
      "Test Precision: 0.9885\n",
      "Test Recall: 0.9865\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class ContinuedEpochMetricsCallback:\n",
    "    def __init__(self, working_path):\n",
    "        self.working_path = working_path\n",
    "        self.epoch_metrics = []\n",
    "        self.epoch_metrics_file = os.path.join(working_path, 'epoch_metrics.json')\n",
    "        \n",
    "    def save_epoch_metrics(self):\n",
    "        with open(self.epoch_metrics_file, 'w') as f:\n",
    "            json.dump(self.epoch_metrics, f, indent=4)\n",
    "    \n",
    "    def on_epoch_start(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        actual_epoch = epoch + 1\n",
    "        \n",
    "        epoch_metrics = {\n",
    "            'global_epoch': actual_epoch,\n",
    "            'local_epoch': epoch + 1,\n",
    "            'duration_minutes': epoch_duration / 60,\n",
    "            'accuracy': logs['accuracy'],\n",
    "            'val_accuracy': logs['val_accuracy'],\n",
    "            'loss': logs['loss'],\n",
    "            'val_loss': logs['val_loss'],\n",
    "            'precision': logs.get('precision', 0),\n",
    "            'val_precision': logs.get('val_precision', 0),\n",
    "            'recall': logs.get('recall', 0),\n",
    "            'val_recall': logs.get('val_recall', 0)\n",
    "        }\n",
    "        \n",
    "        self.epoch_metrics.append(epoch_metrics)\n",
    "        self.save_epoch_metrics()\n",
    "        \n",
    "        print(f\"\\nGlobal Epoch {actual_epoch} Metrics:\")\n",
    "        print(f\"Time taken: {epoch_metrics['duration_minutes']:.2f} minutes\")\n",
    "        print(f\"Training Accuracy: {epoch_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Validation Accuracy: {epoch_metrics['val_accuracy']:.4f}\")\n",
    "        print(f\"Loss: {epoch_metrics['loss']:.4f}\")\n",
    "        print(f\"Validation Loss: {epoch_metrics['val_loss']:.4f}\")\n",
    "        \n",
    "        self.plot_metrics()\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        if not self.epoch_metrics:\n",
    "            return\n",
    "            \n",
    "        epochs = [m['global_epoch'] for m in self.epoch_metrics]\n",
    "        accuracy = [m['accuracy'] for m in self.epoch_metrics]\n",
    "        val_accuracy = [m['val_accuracy'] for m in self.epoch_metrics]\n",
    "        loss = [m['loss'] for m in self.epoch_metrics]\n",
    "        val_loss = [m['val_loss'] for m in self.epoch_metrics]\n",
    "        precision = [m['precision'] for m in self.epoch_metrics]\n",
    "        val_precision = [m['val_precision'] for m in self.epoch_metrics]\n",
    "        recall = [m['recall'] for m in self.epoch_metrics]\n",
    "        val_recall = [m['val_recall'] for m in self.epoch_metrics]\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        axs[0, 0].plot(epochs, accuracy, label='Training Accuracy')\n",
    "        axs[0, 0].plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "        axs[0, 0].set_title('Model Accuracy')\n",
    "        axs[0, 0].set_xlabel('Epoch')\n",
    "        axs[0, 0].set_ylabel('Accuracy')\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        axs[0, 1].plot(epochs, loss, label='Training Loss')\n",
    "        axs[0, 1].plot(epochs, val_loss, label='Validation Loss')\n",
    "        axs[0, 1].set_title('Model Loss')\n",
    "        axs[0, 1].set_xlabel('Epoch')\n",
    "        axs[0, 1].set_ylabel('Loss')\n",
    "        axs[0, 1].legend()\n",
    "\n",
    "        axs[1, 0].plot(epochs, precision, label='Training Precision')\n",
    "        axs[1, 0].plot(epochs, val_precision, label='Validation Precision')\n",
    "        axs[1, 0].set_title('Model Precision')\n",
    "        axs[1, 0].set_xlabel('Epoch')\n",
    "        axs[1, 0].set_ylabel('Precision')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "        axs[1, 1].plot(epochs, recall, label='Training Recall')\n",
    "        axs[1, 1].plot(epochs, val_recall, label='Validation Recall')\n",
    "        axs[1, 1].set_title('Model Recall')\n",
    "        axs[1, 1].set_xlabel('Epoch')\n",
    "        axs[1, 1].set_ylabel('Recall')\n",
    "        axs[1, 1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.working_path, f'training_metrics_epoch_{len(epochs)}.png'))\n",
    "        plt.close()\n",
    "\n",
    "class BirdClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BirdClassifier, self).__init__()\n",
    "        self.efficientnet = models.efficientnet_b0(weights='DEFAULT')\n",
    "        \n",
    "        # Freeze early layers\n",
    "        for param in self.efficientnet.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Unfreeze the last few layers\n",
    "        for layer in [self.efficientnet.features[-1], self.efficientnet.classifier]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "        # Replace the final layer with improved architecture\n",
    "        num_features = self.efficientnet.classifier[1].in_features\n",
    "        self.efficientnet.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)\n",
    "\n",
    "def load_training_state_and_metrics(working_path):\n",
    "    try:\n",
    "        with open(os.path.join(working_path, 'training_state.json'), 'r') as f:\n",
    "            state = json.load(f)\n",
    "            global_epoch = state.get('global_epoch', 0)\n",
    "            best_val_accuracy = state.get('best_val_accuracy', 0.0)\n",
    "            previous_model_path = state.get('model_path', None)\n",
    "    except FileNotFoundError:\n",
    "        global_epoch = 0\n",
    "        best_val_accuracy = 0.0\n",
    "        previous_model_path = None\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(working_path, 'epoch_metrics.json'), 'r') as f:\n",
    "            epoch_metrics = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        epoch_metrics = []\n",
    "    \n",
    "    return global_epoch, best_val_accuracy, previous_model_path, epoch_metrics\n",
    "\n",
    "def save_training_state(working_path, global_epoch, best_val_accuracy, model_path):\n",
    "    state = {\n",
    "        'global_epoch': global_epoch,\n",
    "        'best_val_accuracy': best_val_accuracy,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "    with open(os.path.join(working_path, 'training_state.json'), 'w') as f:\n",
    "        json.dump(state, f, indent=4)\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    return val_loss / len(val_loader), correct / total\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names, device, working_path):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plot_confusion_matrix(cm, class_names, working_path)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                 target_names=class_names, \n",
    "                                 output_dict=True)\n",
    "    \n",
    "    # Save classification report\n",
    "    with open(os.path.join(working_path, 'classification_report.json'), 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    \n",
    "    test_metrics = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': accuracy,\n",
    "        'test_precision': precision,\n",
    "        'test_recall': recall\n",
    "    }\n",
    "    \n",
    "    # Save test metrics\n",
    "    with open(os.path.join(working_path, 'test_metrics.json'), 'w') as f:\n",
    "        json.dump(test_metrics, f, indent=4)\n",
    "    \n",
    "    return test_metrics\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, save_path):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(cm, xticklabels=classes, yticklabels=classes, \n",
    "                annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs, working_path, \n",
    "                global_epoch=0, best_val_accuracy=0.0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',\n",
    "        patience=5,\n",
    "        factor=0.2,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    metrics_callback = ContinuedEpochMetricsCallback(working_path)\n",
    "    best_model_path = os.path.join(working_path, 'best_model.pth')\n",
    "    \n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    min_delta = 0.0005\n",
    "    min_lr_reached_counter = 0\n",
    "    \n",
    "    for epoch in range(global_epoch, num_epochs):\n",
    "        metrics_callback.on_epoch_start()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_val_accuracy + min_delta:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            patience_counter = 0\n",
    "            min_lr_reached_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if current_lr <= 1e-6:\n",
    "            min_lr_reached_counter += 1\n",
    "        else:\n",
    "            min_lr_reached_counter = 0\n",
    "        \n",
    "        should_stop = (\n",
    "            (patience_counter >= patience and min_lr_reached_counter >= 3) or\n",
    "            (patience_counter >= patience * 2)\n",
    "        )\n",
    "        \n",
    "        if should_stop:\n",
    "            print(f\"\\nEarly stopping triggered after epoch {epoch + 1}\")\n",
    "            print(f\"Reason: {'Patience exceeded' if patience_counter >= patience else 'Minimum LR reached' if min_lr_reached_counter >= 3 else 'Target performance reached'}\")\n",
    "            break\n",
    "        \n",
    "        # Log metrics\n",
    "        logs = {\n",
    "            'accuracy': train_acc,\n",
    "            'val_accuracy': val_acc,\n",
    "            'loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'precision': 0,\n",
    "            'val_precision': 0,\n",
    "            'recall': 0,\n",
    "            'val_recall': 0\n",
    "        }\n",
    "        \n",
    "        metrics_callback.on_epoch_end(epoch, logs)\n",
    "        save_training_state(working_path, epoch + 1, best_val_accuracy, best_model_path)\n",
    "    \n",
    "    return best_val_accuracy\n",
    "\n",
    "def main():\n",
    "    BASE_PATH = '/kaggle/input/400birds/400BirdSpecies'\n",
    "    WORKING_PATH = '/kaggle/working/'\n",
    "    TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
    "    VALID_PATH = os.path.join(BASE_PATH, 'valid')\n",
    "    TEST_PATH = os.path.join(BASE_PATH, 'test')\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Optimized data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomAffine(0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = ImageFolder(TRAIN_PATH, transform=train_transform)\n",
    "    val_dataset = ImageFolder(VALID_PATH, transform=val_transform)\n",
    "    test_dataset = ImageFolder(TEST_PATH, transform=val_transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Load previous state\n",
    "    global_epoch, best_val_accuracy, previous_model_path, epoch_metrics = load_training_state_and_metrics(WORKING_PATH)\n",
    "    \n",
    "    print(f\"Resuming training from epoch {global_epoch + 1}\")\n",
    "    print(f\"Previous best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    # Create model\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    model = BirdClassifier(num_classes)\n",
    "    \n",
    "    if os.path.exists(os.path.join(WORKING_PATH, 'best_model.pth')):\n",
    "        model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Store class names\n",
    "    class_names = train_dataset.classes\n",
    "    class_mapping = {i: class_name for i, class_name in enumerate(class_names)}\n",
    "    with open(os.path.join(WORKING_PATH, 'class_mapping.json'), 'w') as f:\n",
    "        json.dump(class_mapping, f, indent=4)\n",
    "    \n",
    "    # Training with 100 epochs max\n",
    "    total_epochs = 100\n",
    "    best_val_accuracy = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        num_epochs=total_epochs,\n",
    "        working_path=WORKING_PATH,\n",
    "        global_epoch=global_epoch,\n",
    "        best_val_accuracy=best_val_accuracy\n",
    "    )\n",
    "    \n",
    "    # Evaluate best model\n",
    "    model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n",
    "    test_metrics = evaluate_model(model, test_loader, class_names, device, WORKING_PATH)\n",
    "    \n",
    "    print(\"\\nTest Set Metrics:\")\n",
    "    print(f\"Test Loss: {test_metrics['test_loss']:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_metrics['test_accuracy']:.4f}\")\n",
    "    print(f\"Test Precision: {test_metrics['test_precision']:.4f}\")\n",
    "    print(f\"Test Recall: {test_metrics['test_recall']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e33b03f",
   "metadata": {
    "papermill": {
     "duration": 0.004499,
     "end_time": "2025-01-20T18:12:48.466207",
     "exception": false,
     "start_time": "2025-01-20T18:12:48.461708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6104137,
     "sourceId": 9960926,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21770.912629,
   "end_time": "2025-01-20T18:12:51.409916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-20T12:10:00.497287",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
