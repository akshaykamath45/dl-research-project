{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "class ContinuedEpochMetricsCallback:\n",
    "    def __init__(self, working_path):\n",
    "        self.working_path = working_path\n",
    "        self.epoch_metrics = []\n",
    "        self.epoch_metrics_file = os.path.join(working_path, 'epoch_metrics.json')\n",
    "        \n",
    "        if os.path.exists(self.epoch_metrics_file):\n",
    "            with open(self.epoch_metrics_file, 'r') as f:\n",
    "                self.epoch_metrics = json.load(f)\n",
    "    \n",
    "    def save_epoch_metrics(self):\n",
    "        with open(self.epoch_metrics_file, 'w') as f:\n",
    "            json.dump(self.epoch_metrics, f, indent=4)\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        if not self.epoch_metrics:\n",
    "            return\n",
    "            \n",
    "        epochs = list(range(1, len(self.epoch_metrics) + 1))\n",
    "        metrics = {\n",
    "            'Accuracy': ('accuracy', 'val_accuracy'),\n",
    "            'Loss': ('loss', 'val_loss'),\n",
    "            'Precision': ('precision', 'val_precision'),\n",
    "            'Recall': ('recall', 'val_recall')\n",
    "        }\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, (title, (train_key, val_key)) in enumerate(metrics.items()):\n",
    "            train_metric = [m[train_key] for m in self.epoch_metrics]\n",
    "            val_metric = [m[val_key] for m in self.epoch_metrics]\n",
    "            \n",
    "            axes[idx].plot(epochs, train_metric, label=f'Training {title}')\n",
    "            axes[idx].plot(epochs, val_metric, label=f'Validation {title}')\n",
    "            axes[idx].set_title(f'Model {title}')\n",
    "            axes[idx].set_xlabel('Epoch')\n",
    "            axes[idx].set_ylabel(title)\n",
    "            axes[idx].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.working_path, f'training_metrics_epoch_{len(epochs)}.png'))\n",
    "        plt.close()\n",
    "\n",
    "class GoogLeNetBirdClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GoogLeNetBirdClassifier, self).__init__()\n",
    "        self.googlenet = models.googlenet(pretrained=True)\n",
    "        \n",
    "        # Freeze all parameters initially\n",
    "        for param in self.googlenet.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Unfreeze the last inception blocks (a3, b3)\n",
    "        layers_to_unfreeze = ['inception5a', 'inception5b']\n",
    "        for name, param in self.googlenet.named_parameters():\n",
    "            if any(layer in name for layer in layers_to_unfreeze):\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Modified classifier head\n",
    "        num_features = self.googlenet.fc.in_features\n",
    "        self.googlenet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.googlenet(x)\n",
    "\n",
    "\n",
    "def load_training_state(working_path):\n",
    "    try:\n",
    "        with open(os.path.join(working_path, 'training_state.json'), 'r') as f:\n",
    "            state = json.load(f)\n",
    "            return (state.get('global_epoch', 0),\n",
    "                   state.get('best_val_accuracy', 0.0),\n",
    "                   state.get('model_path', None))\n",
    "    except FileNotFoundError:\n",
    "        return 0, 0.0, None\n",
    "\n",
    "def save_training_state(working_path, global_epoch, best_val_accuracy, model_path):\n",
    "    state = {\n",
    "        'global_epoch': global_epoch,\n",
    "        'best_val_accuracy': best_val_accuracy,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "    with open(os.path.join(working_path, 'training_state.json'), 'w') as f:\n",
    "        json.dump(state, f, indent=4)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs, working_path, \n",
    "                global_epoch=0, best_val_accuracy=0.0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create parameter groups with different learning rates\n",
    "    # Parameters of unfrozen inception layers\n",
    "    inception_params = []\n",
    "    # Parameters of the classifier (fc layer)\n",
    "    classifier_params = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if 'fc' in name:\n",
    "                classifier_params.append(param)\n",
    "            else:\n",
    "                inception_params.append(param)\n",
    "    \n",
    "    optimizer = optim.Adam([\n",
    "        {'params': inception_params, 'lr': 1e-4},\n",
    "        {'params': classifier_params, 'lr': 2e-4}\n",
    "    ])\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=5, factor=0.2, min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    metrics_callback = ContinuedEpochMetricsCallback(working_path)\n",
    "    best_model_path = os.path.join(working_path, 'best_model.pth')\n",
    "    \n",
    "    for epoch in range(global_epoch, num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Wrap data loader for TPU\n",
    "        para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
    "        \n",
    "        for images, labels in para_train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(optimizer)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        para_val_loader = pl.ParallelLoader(val_loader, [device]).per_device_loader(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in para_val_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            xm.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        # Log metrics\n",
    "        metrics_callback.epoch_metrics.append({\n",
    "            'accuracy': train_acc,\n",
    "            'val_accuracy': val_acc,\n",
    "            'loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'precision': 0,\n",
    "            'val_precision': 0,\n",
    "            'recall': 0,\n",
    "            'val_recall': 0\n",
    "        })\n",
    "        \n",
    "        metrics_callback.save_epoch_metrics()\n",
    "        metrics_callback.plot_metrics()\n",
    "        save_training_state(working_path, epoch + 1, best_val_accuracy, best_model_path)\n",
    "        \n",
    "        xm.master_print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "        xm.master_print(f'Training Loss: {train_loss:.4f}')\n",
    "        xm.master_print(f'Training Accuracy: {train_acc:.4f}')\n",
    "        xm.master_print(f'Validation Loss: {val_loss:.4f}')\n",
    "        xm.master_print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "    \n",
    "    return best_val_accuracy\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names, device, working_path):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    para_test_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in para_test_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Generate and save confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(cm, xticklabels=class_names, yticklabels=class_names, \n",
    "                annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(working_path, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate and save classification report\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                 target_names=class_names, \n",
    "                                 output_dict=True)\n",
    "    \n",
    "    with open(os.path.join(working_path, 'classification_report.json'), 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': accuracy\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    BASE_PATH = '/kaggle/input/400birds/400BirdSpecies'\n",
    "    WORKING_PATH = '/kaggle/working/'\n",
    "    TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
    "    VALID_PATH = os.path.join(BASE_PATH, 'valid')\n",
    "    TEST_PATH = os.path.join(BASE_PATH, 'test')\n",
    "    \n",
    "    # TPU device initialization\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    # Data augmentation and normalization\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomAffine(0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = ImageFolder(TRAIN_PATH, transform=train_transform)\n",
    "    val_dataset = ImageFolder(VALID_PATH, transform=val_transform)\n",
    "    test_dataset = ImageFolder(TEST_PATH, transform=val_transform)\n",
    "    \n",
    "    # Create data loaders optimized for TPU\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=32, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=32, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=32, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Load previous state\n",
    "    global_epoch, best_val_accuracy, _ = load_training_state(WORKING_PATH)\n",
    "    \n",
    "    xm.master_print(f\"Starting training from epoch {global_epoch + 1}\")\n",
    "    xm.master_print(f\"Previous best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    # Create and load model\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    model = GoogLeNetBirdClassifier(num_classes)\n",
    "    \n",
    "    if os.path.exists(os.path.join(WORKING_PATH, 'best_model.pth')):\n",
    "        model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Save class mapping\n",
    "    class_names = train_dataset.classes\n",
    "    class_mapping = {i: name for i, name in enumerate(class_names)}\n",
    "    with open(os.path.join(WORKING_PATH, 'class_mapping.json'), 'w') as f:\n",
    "        json.dump(class_mapping, f, indent=4)\n",
    "    \n",
    "    # Train model\n",
    "    best_val_accuracy = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        num_epochs=100,\n",
    "        working_path=WORKING_PATH,\n",
    "        global_epoch=global_epoch,\n",
    "        best_val_accuracy=best_val_accuracy\n",
    "    )\n",
    "    \n",
    "    # Evaluate best model\n",
    "    model.load_state_dict(torch.load(os.path.join(WORKING_PATH, 'best_model.pth')))\n",
    "    test_metrics = evaluate_model(model, test_loader, class_names, device, WORKING_PATH)\n",
    "    \n",
    "    xm.master_print(\"\\nTest Set Metrics:\")\n",
    "    xm.master_print(f\"Test Loss: {test_metrics['test_loss']:.4f}\")\n",
    "    xm.master_print(f\"Test Accuracy: {test_metrics['test_accuracy']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def _mp_fn(rank, flags):\n",
    "        main()\n",
    "    xmp.spawn(_mp_fn, args=({},), nprocs=1, start_method='fork')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
